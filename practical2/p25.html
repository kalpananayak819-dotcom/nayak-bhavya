<!DOCTYPE html>
<html lang="en">


<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Practical 2.5</title>
</head>


<body bgcolor="#F1E6FA">
    <h1>INDEX</h1>
    <h2><a href="#ai">Artificial intelligence</a></h2>
    <h2><a href="#cs">CyberSecurity</a></h2>
    <h2><a href="#rr">Robotics</a></h2>
    <h2><a href="#it">Information Technology</a></h2>.


    <section id="ai">
        <h1>Artificial intelligence</h1>
    </section>
    <p>Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated
        with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a
        field of research in computer science that develops and studies methods and software that enable machines to
        perceive their environment and use learning and intelligence to take actions that maximize their chances of
        achieving defined goals.[1]


        High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation
        systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa);
        autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and
        superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not
        perceived as AI: "A lot of cutting edge AI has filtered into general applications, often without being called AI
        because once something becomes useful enough and common enough it's not labeled AI anymore."[2][3]


        Various subfields of AI research are centered around particular goals and the use of particular tools. The
        traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural
        language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted
        and integrated a wide range of techniques, including search and mathematical optimization, formal logic,
        artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also
        draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as
        OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI)—AI that can complete
        virtually any cognitive task at least as well as a human.


        Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple
        cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding,
        known as AI winters.[9][10] Funding and interest vastly increased after 2012 when graphics processing units
        started being used to accelerate neural networks and deep learning outperformed previous AI techniques.[11] This
        growth accelerated further after 2017 with the transformer architecture.[12] In the 2020s, an ongoing period of
        rapid progress in advanced generative AI became known as the AI boom. Generative AI's ability to create and
        modify content has led to several unintended consequences and harms, which has raised ethical concerns about
        AI's long-term effects and potential existential risks, prompting discussions about regulatory policies to
        ensure the safety and benefits of the technology.
    </p>
    <section id="cs">
        <h1>CyberSecurity</h1>
    </section>
    <p>Computer security (also cybersecurity, digital security, or information technology (IT) security) is a
        subdiscipline within the field of information security. It focuses on protecting computer software, systems
        and networks from threats that can lead to unauthorized information disclosure, theft or damage to hardware,
        software, or data, as well as from the disruption or misdirection of the services they provide.[1][2]


        The growing significance of computer insecurity reflects the increasing dependence on computer systems, the
        Internet,[3] and evolving wireless network standards. This reliance has expanded with the proliferation of
        smart devices, including smartphones, televisions, and other components of the Internet of things (IoT).


        As digital infrastructure becomes more embedded in everyday life, cybersecurity has emerged as a critical
        concern. The complexity of modern information systems—and the societal functions they underpin—has
        introduced new vulnerabilities. Systems that manage essential services, such as power grids, electoral
        processes, and finance, are particularly sensitive to security breaches.[4][5]




        Although many aspects of computer security involve digital security, such as electronic passwords and
        encryption, physical security measures such as metal locks are still used to prevent unauthorized tampering.
        IT security is not a perfect subset of information security, therefore does not completely align into the
        security convergence schema.


        Vulnerabilities and attacks
        Main article: Vulnerability (computing)
        A vulnerability refers to a flaw in the structure, execution, functioning, or internal oversight of a
        computer or system that compromises its security. Most of the vulnerabilities that have been discovered are
        documented in the Common Vulnerabilities and Exposures (CVE) database.[6] An exploitable vulnerability is
        one for which at least one working attack or exploit exists.[7] Actors maliciously seeking vulnerabilities
        are known as threats. Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using
        automated tools or customized scripts.[8][9]


        Various people or parties are vulnerable to cyber attacks; however, different groups are likely to
        experience different types of attacks more than others.[10]


        In April 2023, the United Kingdom Department for Science, Innovation & Technology released a report on cyber
        attacks over the previous 12 months.[11] They surveyed 2,263 UK businesses, 1,174 UK registered charities,
        and 554 education institutions. The research found that "32% of businesses and 24% of charities overall
        recall any breaches or attacks from the last 12 months." These figures were much higher for "medium
        businesses (59%), large businesses (69%), and high-income charities with £500,000 or more in annual income
        (56%)."[11] Yet, although medium or large businesses are more often the victims, since larger companies have
        generally improved their security over the last decade, small and midsize businesses (SMBs) have also become
        increasingly vulnerable as they often "do not have advanced tools to defend the business."[10] SMBs are most
        likely to be affected by malware, ransomware, phishing, man-in-the-middle attacks, and Denial-of Service
        (DoS) Attacks.[10]


        Normal internet users are most likely to be affected by untargeted cyberattacks.[12] These are where
        attackers indiscriminately target as many devices, services, or users as possible. They do this using
        techniques that take advantage of the openness of the Internet. These strategies mostly include phishing,
        ransomware, water holing and scanning.[12]


        To secure a computer system, it is important to understand the attacks that can be made against it, and
        these threats can typically be classified into one of the following categories:
    </p>
    <section id="rr">
        <h1>Robotics</h1>
    </section>
    <p>Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of
        robots.[1]


        Within mechanical engineering, robotics is the design and construction of the physical structures of robots,
        while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to
        robotics include electrical, control, software, information, electronic, telecommunication, computer,
        mechatronic, and materials engineering.


        The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do
        jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and
        shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning,
        monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances
        continue; researching, designing, and building new robots serve various practical purposes.Robotics is the
        interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]


        Within mechanical engineering, robotics is the design and construction of the physical structures of robots,
        while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to
        robotics include electrical, control, software, information, electronic, telecommunication, computer,
        mechatronic, and materials engineering.


        The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do
        jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and
        shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning,
        monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances
        continue; researching, designing, and building new robots serve various practical purposes.Robotics is the
        interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]


        Within mechanical engineering, robotics is the design and construction of the physical structures of robots,
        while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to
        robotics include electrical, control, software, information, electronic, telecommunication, computer,
        mechatronic, and materials engineering.


        The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do
        jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and
        shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning,
        monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances
        continue; researching, designing, and building new robots serve various practical purposes.
    </p>
    <section id="it">
        <h1>Information Technology</h1>
        <p>Information technology (IT) is the study or use of computers, telecommunication systems and other devices to
            create, process, store, retrieve and transmit information.[1] While the term is commonly used to refer to
            computers and computer networks, it also encompasses other information distribution technologies such as
            television and telephones. Information technology is an application of computer science and computer
            engineering.


            An information technology system (IT system) is generally an information system, a communications system,
            or, more specifically speaking, a computer system — including all hardware, software, and peripheral
            equipment — operated by a limited group of IT users, and an IT project usually refers to the commissioning
            and implementation of an IT system.[2] IT systems play a vital role in facilitating efficient data
            management, enhancing communication networks, and supporting organizational processes across various
            industries. Successful IT projects require meticulous planning and ongoing maintenance to ensure optimal
            functionality and alignment with organizational objectives.[3]


            Although humans have been storing, retrieving, manipulating, analysing and communicating information since
            the earliest writing systems were developed,[4] the term information technology in its modern sense first
            appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L.
            Whisler commented that "the new technology does not yet have a single established name. We shall call it
            information technology (IT)."[5] Their definition consists of three categories: techniques for processing,
            the application of statistical and mathematical methods to decision-making, and the simulation of
            higher-order thinking through computer programs.[5]Information technology (IT) is the study or use of
            computers, telecommunication systems and other devices to create, process, store, retrieve and transmit
            information.[1] While the term is commonly used to refer to computers and computer networks, it also
            encompasses other information distribution technologies such as television and telephones. Information
            technology is an application of computer science and computer engineering.


            An information technology system (IT system) is generally an information system, a communications system,
            or, more specifically speaking, a computer system — including all hardware, software, and peripheral
            equipment — operated by a limited group of IT users, and an IT project usually refers to the commissioning
            and implementation of an IT system.[2] IT systems play a vital role in facilitating efficient data
            management, enhancing communication networks, and supporting organizational processes across various
            industries. Successful IT projects require meticulous planning and ongoing maintenance to ensure optimal
            functionality and alignment with organizational objectives.[3]


            Although humans have been storing, retrieving, manipulating, analysing and communicating information since
            the earliest writing systems were developed,[4] the term information technology in its modern sense first
            appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L.
            Whisler commented that "the new technology does not yet have a single established name. We shall call it
            information technology (IT)."[5] Their definition consists of three categories: techniques for processing,
            the application of statistical and mathematical methods to decision-making, and the simulation of
            higher-order thinking through computer programs.[5]Information technology (IT) is the study or use of
            computers, telecommunication systems and other devices to create, process, store, retrieve and transmit
            information.[1] While the term is commonly used to refer to computers and computer networks, it also
            encompasses other information distribution technologies such as television and telephones. Information
            technology is an application of computer science and computer engineering.


            An information technology system (IT system) is generally an information system, a communications system,
            or, more specifically speaking, a computer system — including all hardware, software, and peripheral
            equipment — operated by a limited group of IT users, and an IT project usually refers to the commissioning
            and implementation of an IT system.[2] IT systems play a vital role in facilitating efficient data
            management, enhancing communication networks, and supporting organizational processes across various
            industries. Successful IT projects require meticulous planning and ongoing maintenance to ensure optimal
            functionality and alignment with organizational objectives.[3]


            Although humans have been storing, retrieving, manipulating, analysing and communicating information since
            the earliest writing systems were developed,[4] the term information technology in its modern sense first
            appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L.
            Whisler commented that "the new technology does not yet have a single established name. We shall call it
            information technology (IT)."[5] Their definition consists of three categories: techniques for processing,
            the application of statistical and mathematical methods to decision-making, and the simulation of
            higher-order thinking through computer programs.[5]Information technology (IT) is the study or use of
            computers, telecommunication systems and other devices to create, process, store, retrieve and transmit
            information.[1] While the term is commonly used to refer to computers and computer networks, it also
            encompasses other information distribution technologies such as television and telephones. Information
            technology is an application of computer science and computer engineering.


            An information technology system (IT system) is generally an information system, a communications system,
            or, more specifically speaking, a computer system — including all hardware, software, and peripheral
            equipment — operated by a limited group of IT users, and an IT project usually refers to the commissioning
            and implementation of an IT system.[2] IT systems play a vital role in facilitating efficient data
            management, enhancing communication networks, and supporting organizational processes across various
            industries. Successful IT projects require meticulous planning and ongoing maintenance to ensure optimal
            functionality and alignment with organizational objectives.[3]


            Although humans have been storing, retrieving, manipulating, analysing and communicating information since
            the earliest writing systems were developed,[4] the term information technology in its modern sense first
            appeared in a 1958 article published in the Harvard Business Review; authors Harold J. Leavitt and Thomas L.
            Whisler commented that "the new technology does not yet have a single established name. We shall call it
            information technology (IT)."[5] Their definition consists of three categories: techniques for processing,
            the application of statistical and mathematical methods to decision-making, and the simulation of
            higher-order thinking through computer programs.[5]
        </p>
</body>


</html>
